---
phase: 04-ai-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/types/ai-messages.ts
  - src/ai/adapters/adapter.ts
  - src/ai/adapters/noop.ts
  - src/ai/router.ts
  - src/ui/signals/store.ts
  - src/types/messages.ts
  - src/worker/worker.ts
autonomous: true
requirements:
  - AINF-01
  - AINF-05
  - AIST-04

must_haves:
  truths:
    - "An AI command dispatched from the UI completes a full round-trip through the adapter router and returns a no-op response that updates the store"
    - "AI provider status (disabled/loading/available/error/unavailable) is tracked in the store and accessible to UI components"
    - "The adapter interface accepts only pre-sanitized string prompts, never raw atom objects — enforced at the type level"
    - "All AI dispatch is user-initiated; no setInterval or autonomous scheduling exists in AI code"
  artifacts:
    - path: "src/types/ai-messages.ts"
      provides: "AI command/response discriminated union types for LLM worker protocol"
      contains: "LLMCommand"
    - path: "src/ai/adapters/adapter.ts"
      provides: "AIAdapter interface, AIRequest, AIResponse, AIProviderStatus types"
      exports: ["AIAdapter", "AIRequest", "AIResponse", "AIProviderStatus"]
    - path: "src/ai/adapters/noop.ts"
      provides: "NoOpAdapter that returns fixed response for round-trip verification"
      exports: ["NoOpAdapter"]
    - path: "src/ai/router.ts"
      provides: "Adapter router that selects active adapter based on store state"
      exports: ["dispatchAI", "setActiveAdapter"]
    - path: "src/ui/signals/store.ts"
      provides: "Extended BinderState with AI fields and derived AI status signals"
      contains: "aiEnabled"
    - path: "src/types/messages.ts"
      provides: "AI_DISPATCH command and AI_RESPONSE response types added to existing protocol"
      contains: "AI_DISPATCH"
  key_links:
    - from: "src/ui/signals/store.ts"
      to: "src/ai/router.ts"
      via: "sendCommand dispatches AI_DISPATCH which calls dispatchAI"
      pattern: "AI_DISPATCH"
    - from: "src/ai/router.ts"
      to: "src/ai/adapters/noop.ts"
      via: "router selects NoOpAdapter and calls execute()"
      pattern: "adapter\\.execute"
    - from: "src/types/messages.ts"
      to: "src/ui/signals/store.ts"
      via: "AI_RESPONSE updates aiActivity and provider status in store"
      pattern: "AI_RESPONSE"
---

<objective>
Establish the AI backbone: typed message protocol, pluggable adapter interface, no-op adapter, AI command router, and store extension with AI state fields. Verify end-to-end round-trip from UI dispatch through adapter to store update.

Purpose: Every subsequent AI plan (LLM worker, cloud adapter, settings UI) plugs into this foundation. The no-op adapter proves the full pipeline works before any real model is connected.

Output: Working AI dispatch pipeline verified with no-op adapter; store tracks AI provider status; adapter interface enforces privacy boundary at the type level.
</objective>

<execution_context>
@C:/Users/patri/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/patri/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ai-infrastructure/04-RESEARCH.md

@src/types/messages.ts
@src/ui/signals/store.ts
@src/worker/worker.ts
@src/worker/bridge.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AI type system, adapter interface, no-op adapter, and router</name>
  <files>
    src/types/ai-messages.ts
    src/ai/adapters/adapter.ts
    src/ai/adapters/noop.ts
    src/ai/router.ts
  </files>
  <action>
**1. Create `src/types/ai-messages.ts`** — LLM worker message protocol (discriminated unions mirroring `src/types/messages.ts` pattern):

```typescript
export type LLMCommand =
  | { type: 'LLM_INIT' }
  | { type: 'LLM_REQUEST'; payload: { requestId: string; prompt: string; maxTokens?: number } }
  | { type: 'LLM_ABORT'; payload: { requestId: string } };

export type LLMResponse =
  | { type: 'LLM_READY'; payload: { modelId: string; device: 'webgpu' | 'wasm'; tier: 'fast' | 'quality' } }
  | { type: 'LLM_PROGRESS'; payload: { requestId: string; chunk: string } }
  | { type: 'LLM_COMPLETE'; payload: { requestId: string; text: string } }
  | { type: 'LLM_STATUS'; payload: { status: AIProviderStatus; modelId?: string; device?: string } }
  | { type: 'LLM_ERROR'; payload: { requestId?: string; message: string } }
  | { type: 'LLM_DOWNLOAD_PROGRESS'; payload: { progress: number; loaded: number; total: number } };
```

Import `AIProviderStatus` from `../ai/adapters/adapter.ts`.

**2. Create `src/ai/adapters/adapter.ts`** — Core interface types:

- `AIProviderStatus` = `'disabled' | 'loading' | 'available' | 'error' | 'unavailable'`
- `AIRequest` with fields: `requestId: string`, `prompt: string` (ALWAYS pre-sanitized — never raw atom data), `maxTokens?: number`, `onChunk?: (chunk: string) => void`, `signal?: AbortSignal`
- `AIResponse` with fields: `requestId: string`, `text: string`, `provider: 'noop' | 'browser' | 'cloud'`, `model?: string`
- `AIAdapter` interface: `readonly id: 'noop' | 'browser' | 'cloud'`, `readonly status: AIProviderStatus`, `execute(request: AIRequest): Promise<AIResponse>`, `dispose(): void`

Add JSDoc on `AIRequest.prompt`: "ALWAYS pre-sanitized string — never raw atom data. Cloud adapters enforce this at the type level per privacy proxy architecture."

**3. Create `src/ai/adapters/noop.ts`** — NoOpAdapter:

- Implements `AIAdapter` with `id = 'noop'`, `status = 'available'`
- `execute()`: `await new Promise(resolve => setTimeout(resolve, 50))` then calls `request.onChunk?.('[no-op response]')` and returns `{ requestId: request.requestId, text: '[no-op response]', provider: 'noop' }`
- `dispose()`: no-op

**4. Create `src/ai/router.ts`** — Adapter router:

- Holds a module-level `activeAdapter: AIAdapter | null` initialized to `null`
- `setActiveAdapter(adapter: AIAdapter | null): void` — sets the active adapter
- `getActiveAdapter(): AIAdapter | null` — returns current adapter
- `async dispatchAI(request: AIRequest): Promise<AIResponse>` — if no adapter or adapter.status !== 'available', throw `new Error('No AI adapter available')`. Otherwise call `adapter.execute(request)`.
- The router does NOT do any autonomous scheduling (AIST-04). It is always called as a result of user action.
  </action>
  <verify>
Run `npx tsc --noEmit` — all four new files compile without errors. Verify `AIRequest.prompt` is typed as `string` (not `Atom` or any atom type).
  </verify>
  <done>
Four files exist with correct types. AIAdapter interface has execute/dispose/status/id. NoOpAdapter implements the interface. Router dispatches to active adapter. No autonomous scheduling anywhere.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend store, messages, and worker with AI state and AI_DISPATCH command</name>
  <files>
    src/ui/signals/store.ts
    src/types/messages.ts
    src/worker/worker.ts
  </files>
  <action>
**1. Extend `src/ui/signals/store.ts`:**

Add to `BinderState` interface (after Phase 3 fields, with comment `// Phase 4: AI infrastructure`):
```typescript
aiEnabled: boolean;
browserLLMEnabled: boolean;
cloudAPIEnabled: boolean;
llmStatus: AIProviderStatus;
cloudStatus: AIProviderStatus;
llmModelId: string | null;
llmDevice: 'webgpu' | 'wasm' | null;
llmDownloadProgress: number | null;
aiActivity: string | null;
aiFirstRunComplete: boolean;
```

Import `AIProviderStatus` from `../../ai/adapters/adapter`.

Add to `initialState` (all AI disabled by default per AIST-01):
```typescript
aiEnabled: false,
browserLLMEnabled: false,
cloudAPIEnabled: false,
llmStatus: 'disabled',
cloudStatus: 'disabled',
llmModelId: null,
llmDevice: null,
llmDownloadProgress: null,
aiActivity: null,
aiFirstRunComplete: false,
```

Add derived signals after Phase 3 signals:
```typescript
// Phase 4: AI derived signals
export const llmReady = createMemo(() => state.llmStatus === 'available');
export const cloudReady = createMemo(() => state.cloudStatus === 'available');
export const anyAIAvailable = createMemo(() => llmReady() || cloudReady());
```

Add new response cases in the `onMessage` handler:
- `case 'AI_RESPONSE'`: Update `setState('aiActivity', null)` (clear activity on completion). If `response.payload.llmStatus` exists, update it. If `response.payload.cloudStatus` exists, update it.
- `case 'AI_STATUS'`: Update AI status fields from payload (llmStatus, cloudStatus, llmModelId, llmDevice, llmDownloadProgress, aiActivity).

Add UI state setter:
```typescript
export function setAIEnabled(enabled: boolean): void {
  setState('aiEnabled', enabled);
}
export function setBrowserLLMEnabled(enabled: boolean): void {
  setState('browserLLMEnabled', enabled);
}
export function setCloudAPIEnabled(enabled: boolean): void {
  setState('cloudAPIEnabled', enabled);
}
export function setAIFirstRunComplete(complete: boolean): void {
  setState('aiFirstRunComplete', complete);
}
```

**2. Extend `src/types/messages.ts`:**

Add to `Command` union:
```typescript
| { type: 'AI_DISPATCH'; payload: { requestId: string; prompt: string; maxTokens?: number } }
```

Add to `Response` union:
```typescript
| { type: 'AI_RESPONSE'; payload: { requestId: string; text: string; provider: 'noop' | 'browser' | 'cloud'; model?: string; llmStatus?: AIProviderStatus; cloudStatus?: AIProviderStatus } }
| { type: 'AI_STATUS'; payload: { llmStatus?: AIProviderStatus; cloudStatus?: AIProviderStatus; llmModelId?: string | null; llmDevice?: 'webgpu' | 'wasm' | null; llmDownloadProgress?: number | null; aiActivity?: string | null } }
```

Import `AIProviderStatus` from `../ai/adapters/adapter`.

**3. Extend `src/worker/worker.ts`:**

Add import for `dispatchAI`, `setActiveAdapter` from `../ai/router` and `NoOpAdapter` from `../ai/adapters/noop`.

In the `INIT` case, after the existing init code, initialize the no-op adapter:
```typescript
// Phase 4: Initialize AI with no-op adapter for round-trip verification
import { NoOpAdapter } from '../ai/adapters/noop';
import { setActiveAdapter } from '../ai/router';
setActiveAdapter(new NoOpAdapter());
```

(Move imports to top of file, not inline.)

Add new case in the switch:
```typescript
case 'AI_DISPATCH': {
  // Phase 4: AI dispatch is always user-initiated (AIST-04)
  setState with aiActivity
  const aiResponse: Response = { type: 'AI_STATUS', payload: { aiActivity: 'Processing...' } };
  self.postMessage(aiResponse);

  const result = await dispatchAI({
    requestId: msg.payload.requestId,
    prompt: msg.payload.prompt,
    maxTokens: msg.payload.maxTokens,
  });

  const response: Response = {
    type: 'AI_RESPONSE',
    payload: {
      requestId: result.requestId,
      text: result.text,
      provider: result.provider,
      model: result.model,
    },
  };
  self.postMessage(response);

  // Clear activity
  const clearActivity: Response = { type: 'AI_STATUS', payload: { aiActivity: null } };
  self.postMessage(clearActivity);
  break;
}
```

**Important:** Update the exhaustiveness check `default` block — adding AI_DISPATCH to the Command union means the `never` check in the default case will fail to compile if the case is missing.
  </action>
  <verify>
Run `npx tsc --noEmit` — all files compile. Verify the store has all 10 new AI fields. Verify the worker handles AI_DISPATCH and initializes the NoOpAdapter on INIT. Run `pnpm lint` to check for solid/reactivity warnings on the new derived signals.
  </verify>
  <done>
Store extended with 10 AI fields, 3 derived signals, 4 setters, and 2 new response handlers. Messages extended with AI_DISPATCH command and AI_RESPONSE/AI_STATUS responses. Worker initializes NoOpAdapter on INIT and dispatches AI_DISPATCH through the router. Full round-trip: sendCommand({type: 'AI_DISPATCH', payload: {requestId, prompt}}) -> worker -> router -> NoOpAdapter -> AI_RESPONSE -> store update.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. `pnpm lint` passes (no solid/reactivity warnings on new memos)
3. Manual verification: the `BinderState` interface has fields `aiEnabled`, `browserLLMEnabled`, `cloudAPIEnabled`, `llmStatus`, `cloudStatus`, `llmModelId`, `llmDevice`, `llmDownloadProgress`, `aiActivity`, `aiFirstRunComplete`
4. The `Command` type includes `AI_DISPATCH` and the `Response` type includes `AI_RESPONSE` and `AI_STATUS`
5. The worker switch statement handles `AI_DISPATCH` and the default exhaustiveness check still compiles
6. `NoOpAdapter` is instantiated in the INIT handler and set via `setActiveAdapter`
7. No `setInterval` or autonomous AI scheduling exists in any new code (AIST-04)
</verification>

<success_criteria>
- AI type system compiles: adapter interface, request/response types, LLM worker protocol
- No-op adapter passes round-trip: UI dispatch -> worker -> router -> NoOpAdapter -> AI_RESPONSE -> store
- Store reflects AI state: all 10 fields initialized to disabled/null defaults
- Derived signals work: llmReady, cloudReady, anyAIAvailable are reactive memos
- Privacy boundary enforced: AIRequest.prompt is string, not Atom
- No autonomous scheduling: all AI dispatch is in response to user commands
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-infrastructure/04-01-SUMMARY.md`
</output>
