---
phase: 04-ai-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/worker/llm-worker.ts
  - src/worker/llm-bridge.ts
  - src/ai/adapters/browser.ts
  - vite.config.ts
autonomous: true
requirements:
  - AINF-02
  - AINF-03
  - AINF-06

must_haves:
  truths:
    - "A dedicated LLM worker runs SmolLM2 via Transformers.js in isolation from the BinderCore worker — no shared thread"
    - "On a WebGPU-capable machine, the larger model variant is selected automatically; on CPU-only, the smaller WASM model is used"
    - "Browser LLM works fully offline after initial model download; cloud features show a friendly unavailable message when offline"
    - "Model download progress is reported to the main thread and reflected in the store"
  artifacts:
    - path: "src/worker/llm-worker.ts"
      provides: "Dedicated LLM Web Worker with SmolLM2 pipeline, WebGPU detection, model download progress"
      contains: "pipeline"
    - path: "src/worker/llm-bridge.ts"
      provides: "Main-thread bridge for LLM worker (mirrors bridge.ts pattern)"
      exports: ["initLLMWorker", "dispatchLLM", "onLLMMessage", "terminateLLMWorker"]
    - path: "src/ai/adapters/browser.ts"
      provides: "BrowserAdapter that routes AI requests through the LLM bridge"
      exports: ["BrowserAdapter"]
    - path: "vite.config.ts"
      provides: "Updated Vite config with cross-origin isolation headers for SharedArrayBuffer"
      contains: "Cross-Origin"
  key_links:
    - from: "src/ai/adapters/browser.ts"
      to: "src/worker/llm-bridge.ts"
      via: "BrowserAdapter.execute() calls dispatchLLM()"
      pattern: "dispatchLLM"
    - from: "src/worker/llm-bridge.ts"
      to: "src/worker/llm-worker.ts"
      via: "postMessage LLMCommand to dedicated worker"
      pattern: "worker\\.postMessage"
    - from: "src/worker/llm-worker.ts"
      to: "@huggingface/transformers"
      via: "pipeline('text-generation', modelId, { device })"
      pattern: "pipeline.*text-generation"
    - from: "src/ai/adapters/browser.ts"
      to: "src/ui/signals/store.ts"
      via: "BrowserAdapter.onStatusChange callback calls setState for llmStatus, llmDevice, llmModelId, llmDownloadProgress"
      pattern: "onStatusChange"
---

<objective>
Build the dedicated LLM Web Worker running SmolLM2 via Transformers.js with WebGPU-tiered model selection, model download progress reporting, and offline detection. Create the bridge and browser adapter that plug into the Plan 01 adapter interface.

Purpose: This is the local/private AI inference layer. All browser LLM features depend on this worker being isolated from BinderCore (prevents OOM crashes and unblocks atom mutations during inference). WebGPU detection enables automatic quality tiering.

Output: Working LLM worker that initializes SmolLM2, detects WebGPU/WASM, reports download progress, handles text generation requests, and integrates with the adapter router via BrowserAdapter.
</objective>

<execution_context>
@C:/Users/patri/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/patri/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ai-infrastructure/04-RESEARCH.md
@.planning/phases/04-ai-infrastructure/04-01-SUMMARY.md

@src/worker/worker.ts
@src/worker/bridge.ts
@src/types/ai-messages.ts
@src/ai/adapters/adapter.ts
@src/ai/router.ts
@vite.config.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM worker and bridge with WebGPU detection and model download</name>
  <files>
    src/worker/llm-worker.ts
    src/worker/llm-bridge.ts
    vite.config.ts
  </files>
  <action>
**1. Create `src/worker/llm-worker.ts`** — Dedicated LLM inference worker:

Follow the exact pattern of `src/worker/worker.ts` (self.onmessage handler with switch on type). Import types from `../types/ai-messages.ts`.

**WebGPU Detection (inside the worker, not main thread per RESEARCH.md anti-pattern):**
```typescript
import { pipeline, env } from '@huggingface/transformers';

env.allowLocalModels = false; // Use remote Hub models only

const MODEL_TIERS = {
  webgpu: 'HuggingFaceTB/SmolLM2-360M-Instruct',  // "Quality"
  wasm:   'HuggingFaceTB/SmolLM2-135M-Instruct',   // "Fast"
} as const;

async function detectDevice(): Promise<'webgpu' | 'wasm'> {
  if (!env.apis.IS_WEBGPU_AVAILABLE) return 'wasm';
  try {
    const adapter = await navigator.gpu.requestAdapter();
    return adapter !== null ? 'webgpu' : 'wasm';
  } catch {
    return 'wasm';
  }
}
```

**Model initialization with progress:**
```typescript
let generator: Awaited<ReturnType<typeof pipeline>> | null = null;

async function initModel(modelId: string, device: 'webgpu' | 'wasm') {
  generator = await pipeline('text-generation', modelId, {
    device,
    dtype: device === 'webgpu' ? 'fp16' : 'q8',
    progress_callback: (progress: { progress?: number; loaded?: number; total?: number }) => {
      self.postMessage({
        type: 'LLM_DOWNLOAD_PROGRESS',
        payload: {
          progress: Math.round(progress.progress ?? 0),
          loaded: progress.loaded ?? 0,
          total: progress.total ?? 0,
        },
      });
    },
  });
}
```

**Message handler:**
- `LLM_INIT`: Detect device, select model from `MODEL_TIERS`, send `LLM_STATUS` with `status: 'loading'`, call `initModel()`, send `LLM_READY` with `modelId`, `device`, `tier`.
- `LLM_REQUEST`: If `generator` is null, send `LLM_ERROR`. Otherwise call `generator(prompt, { max_new_tokens: maxTokens ?? 256 })`. Extract generated text. Send `LLM_COMPLETE` with the result. Wrap in try/catch — on error send `LLM_ERROR`.
- `LLM_ABORT`: Store abort controllers in a `Map<string, AbortController>` keyed by requestId. On abort, call `controller.abort()`. (Note: Transformers.js pipeline does not natively support abort, so this is a best-effort — if the request is still in the pipeline queue, remove it; if executing, it will complete.)

**Error handling:** Wrap entire `self.onmessage` in try/catch. On unhandled error, send `LLM_ERROR` with `message`.

**2. Create `src/worker/llm-bridge.ts`** — Main-thread bridge for LLM worker:

Mirror the `src/worker/bridge.ts` pattern exactly:

```typescript
import type { LLMCommand, LLMResponse } from '../types/ai-messages';

let worker: Worker | null = null;
let messageHandler: ((response: LLMResponse) => void) | null = null;

export function initLLMWorker(): Promise<LLMResponse> {
  worker = new Worker(new URL('./llm-worker.ts', import.meta.url), {
    type: 'module',
  });

  return new Promise((resolve, reject) => {
    if (!worker) return reject(new Error('LLM Worker failed to create'));

    worker.onmessage = (event: MessageEvent<LLMResponse>) => {
      const response = event.data;

      if (response.type === 'LLM_READY') {
        // Swap to persistent handler
        worker!.onmessage = (evt: MessageEvent<LLMResponse>) => {
          messageHandler?.(evt.data);
        };
        messageHandler?.(response); // Forward READY to handler
        resolve(response);
      } else if (response.type === 'LLM_ERROR') {
        reject(new Error(response.payload.message));
      } else {
        // Forward download progress etc. during init
        messageHandler?.(response);
      }
    };

    worker.onerror = (err) => reject(new Error(`LLM Worker error: ${err.message}`));

    dispatchLLM({ type: 'LLM_INIT' });
  });
}

export function dispatchLLM(command: LLMCommand): void {
  worker?.postMessage(command);
}

export function onLLMMessage(handler: (response: LLMResponse) => void): void {
  messageHandler = handler;
}

export function terminateLLMWorker(): void {
  worker?.terminate();
  worker = null;
  messageHandler = null;
}
```

**3. Update `vite.config.ts`** — Add cross-origin isolation headers for SharedArrayBuffer (needed by ONNX WASM backend per RESEARCH.md Pitfall 4):

Add to the `defineConfig` object:
```typescript
server: {
  headers: {
    'Cross-Origin-Embedder-Policy': 'require-corp',
    'Cross-Origin-Opener-Policy': 'same-origin',
  },
},
```

These headers go at the top level of `defineConfig`, not inside `plugins`. This enables `SharedArrayBuffer` in the dev server which ONNX WASM may need. Also add the same headers for `preview`:
```typescript
preview: {
  headers: {
    'Cross-Origin-Embedder-Policy': 'require-corp',
    'Cross-Origin-Opener-Policy': 'same-origin',
  },
},
```

**Important caveats documented in code comments:**
- Dev mode may re-download models on full reload (RESEARCH.md Pitfall 2) — this is expected
- `navigator.storage.persist()` already called by existing BinderCore persistence module — do NOT add a second call (RESEARCH.md Open Question 2)
- The LLM worker is a DedicatedWorker, NOT a ServiceWorker (RESEARCH.md Pitfall 1)
  </action>
  <verify>
Run `npx tsc --noEmit` — all new files compile. Verify `llm-worker.ts` imports from `@huggingface/transformers` (already in package.json). Verify `llm-bridge.ts` creates worker with `new Worker(new URL('./llm-worker.ts', import.meta.url), { type: 'module' })`. Verify `vite.config.ts` has COOP/COEP headers in both `server` and `preview` sections.
  </verify>
  <done>
LLM worker file exists with WebGPU detection, tiered model selection, pipeline initialization with progress, and LLMCommand/LLMResponse message handling. Bridge creates and manages the worker. Vite config has cross-origin isolation headers.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create BrowserAdapter with offline detection and wire into worker dispatch</name>
  <files>
    src/ai/adapters/browser.ts
    src/worker/worker.ts
  </files>
  <action>
**1. Create `src/ai/adapters/browser.ts`** — BrowserAdapter implementation:

Implements `AIAdapter` interface from `./adapter.ts`. Routes requests through the LLM bridge.

```typescript
import type { AIAdapter, AIRequest, AIResponse, AIProviderStatus } from './adapter';
import { dispatchLLM, onLLMMessage, initLLMWorker, terminateLLMWorker } from '../../worker/llm-bridge';
import type { LLMResponse } from '../../types/ai-messages';

export class BrowserAdapter implements AIAdapter {
  readonly id = 'browser' as const;
  private _status: AIProviderStatus = 'disabled';
  private pendingRequests = new Map<string, {
    resolve: (response: AIResponse) => void;
    reject: (error: Error) => void;
    onChunk?: (chunk: string) => void;
  }>();

  get status(): AIProviderStatus {
    return this._status;
  }

  // Optional callback for forwarding status changes to the store.
  // Set by the caller (e.g., store initialization code) before calling initialize().
  onStatusChange?: (update: {
    status?: AIProviderStatus;
    device?: string;
    modelId?: string;
    downloadProgress?: number | null;
  }) => void;

  async initialize(): Promise<void> {
    this._status = 'loading';
    this.onStatusChange?.({ status: 'loading' });

    // Set up message handler BEFORE init so download progress is captured
    onLLMMessage((response: LLMResponse) => {
      this.handleWorkerMessage(response);
    });

    try {
      await initLLMWorker();
      this._status = 'available';
      this.onStatusChange?.({ status: 'available' });
    } catch (err) {
      this._status = 'error';
      this.onStatusChange?.({ status: 'error' });
      throw err;
    }
  }

  private handleWorkerMessage(response: LLMResponse): void {
    switch (response.type) {
      case 'LLM_COMPLETE': {
        const pending = this.pendingRequests.get(response.payload.requestId);
        if (pending) {
          this.pendingRequests.delete(response.payload.requestId);
          pending.resolve({
            requestId: response.payload.requestId,
            text: response.payload.text,
            provider: 'browser',
          });
        }
        break;
      }
      case 'LLM_PROGRESS': {
        const pending = this.pendingRequests.get(response.payload.requestId);
        pending?.onChunk?.(response.payload.chunk);
        break;
      }
      case 'LLM_ERROR': {
        if (response.payload.requestId) {
          const pending = this.pendingRequests.get(response.payload.requestId);
          if (pending) {
            this.pendingRequests.delete(response.payload.requestId);
            pending.reject(new Error(response.payload.message));
          }
        }
        break;
      }
      case 'LLM_STATUS': {
        this._status = response.payload.status;
        this.onStatusChange?.({ status: response.payload.status });
        break;
      }
      case 'LLM_READY': {
        // Forward model identity to the store so StatusBar and AISettingsPanel can display it
        this.onStatusChange?.({
          status: 'available',
          modelId: response.payload.modelId,
          device: response.payload.device,
          downloadProgress: null, // download complete
        });
        break;
      }
      case 'LLM_DOWNLOAD_PROGRESS': {
        // Forward download progress to the store for the progress bar
        this.onStatusChange?.({
          downloadProgress: response.payload.progress,
        });
        break;
      }
    }
  }

  async execute(request: AIRequest): Promise<AIResponse> {
    if (this._status !== 'available') {
      throw new Error('Browser LLM not available');
    }

    return new Promise<AIResponse>((resolve, reject) => {
      this.pendingRequests.set(request.requestId, {
        resolve,
        reject,
        onChunk: request.onChunk,
      });

      dispatchLLM({
        type: 'LLM_REQUEST',
        payload: {
          requestId: request.requestId,
          prompt: request.prompt,
          maxTokens: request.maxTokens,
        },
      });

      // Handle abort signal
      if (request.signal) {
        request.signal.addEventListener('abort', () => {
          dispatchLLM({ type: 'LLM_ABORT', payload: { requestId: request.requestId } });
          this.pendingRequests.delete(request.requestId);
          reject(new Error('Request aborted'));
        });
      }
    });
  }

  dispose(): void {
    terminateLLMWorker();
    this.pendingRequests.clear();
    this._status = 'disabled';
  }
}
```

**Offline detection (AINF-06):** Add to the adapter:
```typescript
private offlineHandler = () => {
  // Browser LLM works offline — don't change browser adapter status
  // Cloud adapter will handle its own offline status separately
};
```

Add a static utility function (used by other code to check offline for cloud features):
```typescript
export function isOnline(): boolean {
  return navigator.onLine;
}

// Register for online/offline events (call once at app startup)
export function registerOnlineListener(onOffline: () => void, onOnline: () => void): () => void {
  window.addEventListener('offline', onOffline);
  window.addEventListener('online', onOnline);
  return () => {
    window.removeEventListener('offline', onOffline);
    window.removeEventListener('online', onOnline);
  };
}
```

**2. Update `src/worker/worker.ts`** — Wire BrowserAdapter into LLM worker message forwarding:

The BinderCore worker does NOT import BrowserAdapter directly (that would pull transformers.js into the BinderCore worker). Instead, update the AI_DISPATCH handler to also handle the case where browserLLM is the active adapter. The BrowserAdapter lives on the main thread (it manages its own worker via llm-bridge.ts).

Actually, looking at the architecture more carefully: the router and adapters live on the **main thread**, not in the BinderCore worker. The AI_DISPATCH command in the BinderCore worker from Plan 01 needs to be reconsidered.

**Architecture correction:** Move AI dispatch to the main thread. The store's `sendCommand` for `AI_DISPATCH` should NOT go through the BinderCore worker. Instead:
- Add a new function `dispatchAICommand` in `store.ts` that directly calls the router (it's on the main thread)
- The BinderCore worker does NOT need to handle AI_DISPATCH at all
- Remove `AI_DISPATCH` from the BinderCore worker switch statement (it was added in Plan 01 — this plan corrects the architecture)
- Keep `AI_DISPATCH` in the Command type for potential future use, but add a guard comment
- Add `AI_RESPONSE` and `AI_STATUS` to the store handler so the LLM bridge messages flow correctly

Update the `store.ts` to add:
```typescript
import { dispatchAI } from '../../ai/router';
import type { AIRequest } from '../../ai/adapters/adapter';

export async function dispatchAICommand(prompt: string, maxTokens?: number): Promise<void> {
  const requestId = crypto.randomUUID();
  setState('aiActivity', 'Processing...');
  try {
    const result = await dispatchAI({ requestId, prompt, maxTokens });
    // AI_RESPONSE-like behavior — process result
    setState('aiActivity', null);
    // Further result handling will be added in Phase 5 (triage, review)
  } catch (err) {
    setState('aiActivity', null);
    setState('lastError', err instanceof Error ? err.message : String(err));
  }
}
```

**Wire BrowserAdapter status changes into the store** — In the store initialization code (or wherever the BrowserAdapter is instantiated), set the `onStatusChange` callback so that LLM worker status updates flow reactively to the UI:

```typescript
import { BrowserAdapter } from '../../ai/adapters/browser';

// When BrowserAdapter is created (e.g., in initAI() or when browserLLMEnabled becomes true):
const browserAdapter = new BrowserAdapter();

browserAdapter.onStatusChange = (update) => {
  if (update.status !== undefined) setState('llmStatus', update.status);
  if (update.device !== undefined) setState('llmDevice', update.device);
  if (update.modelId !== undefined) setState('llmModelId', update.modelId);
  if (update.downloadProgress !== undefined) setState('llmDownloadProgress', update.downloadProgress);
};

await browserAdapter.initialize();
```

This ensures that `LLM_STATUS`, `LLM_READY`, and `LLM_DOWNLOAD_PROGRESS` messages from the LLM worker are forwarded through the bridge → BrowserAdapter → onStatusChange callback → setState(), making the store fields `llmStatus`, `llmDevice`, `llmModelId`, and `llmDownloadProgress` reactive. The StatusBar and AISettingsPanel will update automatically.
```

In `worker.ts`, remove the `AI_DISPATCH` case from the switch (move the comment `// Phase 4: AI dispatch handled on main thread via dispatchAICommand in store.ts`). To maintain exhaustiveness, update the Command type or add AI_DISPATCH to the switch with a comment that it's handled main-thread-side and just `break`.
  </action>
  <verify>
Run `npx tsc --noEmit` — all files compile. Verify `BrowserAdapter` implements `AIAdapter`. Verify `isOnline()` and `registerOnlineListener()` are exported from browser.ts. Verify `dispatchAICommand` exists in store.ts and calls the router directly. Verify `worker.ts` does not import any AI adapter code (no Transformers.js contamination in BinderCore worker).
  </verify>
  <done>
BrowserAdapter routes AI requests through the LLM bridge to the dedicated LLM worker. Offline detection utilities are exported. AI dispatch happens on the main thread (not in BinderCore worker). LLM worker is completely isolated. Architecture correctly separates: BinderCore worker (atom mutations) | main thread (adapter router, store) | LLM worker (inference).
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes
2. `pnpm lint` passes
3. `llm-worker.ts` imports from `@huggingface/transformers` and does NOT import from `../storage/db` or `../wasm/` (complete isolation from BinderCore)
4. `llm-bridge.ts` creates worker with `type: 'module'` and handles init/dispatch/terminate
5. `browser.ts` implements `AIAdapter` and tracks pending requests with requestId map
6. `vite.config.ts` has COOP/COEP headers in both `server` and `preview`
7. BinderCore `worker.ts` does NOT import Transformers.js or any adapter code
8. `navigator.onLine` check is available via `isOnline()` export
</verification>

<success_criteria>
- LLM worker initializes SmolLM2 with WebGPU or WASM based on hardware detection
- Model download progress flows: llm-worker -> llm-bridge -> BrowserAdapter -> store
- BrowserAdapter.execute() returns AI responses routed through the LLM worker
- Browser LLM status is 'available' after model loads, remains available when offline
- BinderCore worker thread never loads Transformers.js (separate process isolation)
- Vite dev server supports SharedArrayBuffer via cross-origin isolation headers
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-infrastructure/04-02-SUMMARY.md`
</output>
