---
phase: 06-review-pre-analysis
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/ai/llm-worker.ts
  - src/ai/adapters/browser.ts
  - src/ui/components/AISettingsPanel.tsx
  - src/storage/ai-settings.ts
  - package.json
autonomous: true
requirements:
  - AIRV-01
  - AIRV-02
  - AIGN-01
  - AIRV-05

must_haves:
  truths:
    - "Local AI uses WebLLM with Llama-3.2-3B-Instruct instead of Transformers.js with SmolLM2"
    - "WebLLM's response_format with JSON schema constraint produces valid structured JSON"
    - "AI Settings panel shows a model size selector (1B / 3B / 3.8B) with VRAM guidance"
    - "Model download progress is reported to the UI during initial WebLLM model load"
    - "WebGPU is required — no CPU fallback (WebLLM does not support WASM inference)"
  artifacts:
    - path: "src/ai/llm-worker.ts"
      provides: "WebLLM-powered LLM worker using WebWorkerMLCEngine"
      contains: "web-llm"
    - path: "src/ai/adapters/browser.ts"
      provides: "BrowserAdapter using WebLLM OpenAI-compatible API"
      contains: "chat.completions.create"
    - path: "src/ui/components/AISettingsPanel.tsx"
      provides: "Model selector dropdown with VRAM guidance"
      contains: "model-selector"
    - path: "src/storage/ai-settings.ts"
      provides: "selectedModelId field in AISettings"
      contains: "selectedModelId"
  key_links:
    - from: "src/ai/adapters/browser.ts"
      to: "src/ai/llm-worker.ts"
      via: "WebWorkerMLCEngine communicates with worker"
      pattern: "WebWorkerMLCEngine|MLCEngine"
    - from: "src/ui/components/AISettingsPanel.tsx"
      to: "src/storage/ai-settings.ts"
      via: "model selection persisted via saveAISettings"
      pattern: "selectedModelId"
---

<objective>
Replace the Transformers.js + SmolLM2 local AI engine with WebLLM (@mlc-ai/web-llm), rewriting the LLM worker and browser adapter to use WebLLM's OpenAI-compatible API with structured JSON output. Add a model size selector to the AI Settings panel.

Purpose: Solves the Phase 5 limitation where SmolLM2 could not produce reliable structured JSON. WebLLM's XGrammar-based constrained generation guarantees valid JSON output. Larger models (3B params vs 360M) dramatically improve instruction following for triage and review analysis. This migration supports AIRV-01 and AIRV-02 by enabling local AI to participate in review analysis with structured output.

Output: Rewritten llm-worker.ts using WebWorkerMLCEngine, rewritten browser.ts adapter using WebLLM API, model selector in AI settings, updated AISettings type.
</objective>

<execution_context>
@C:/Users/patri/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/patri/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-review-pre-analysis/06-CONTEXT.md
@.planning/phases/06-review-pre-analysis/06-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace Transformers.js with WebLLM in llm-worker + browser adapter</name>
  <files>
    src/ai/llm-worker.ts
    src/ai/adapters/browser.ts
    package.json
  </files>
  <action>
**1. Install WebLLM and remove Transformers.js (`package.json`):**

Run:
```bash
pnpm remove @xenova/transformers
pnpm add @mlc-ai/web-llm@0.2.81
```

If `@xenova/transformers` is not a direct dependency (may be `@huggingface/transformers`), check package.json first and remove the correct package name.

**2. Rewrite LLM worker (`src/ai/llm-worker.ts`):**

The current worker uses Transformers.js pipeline API. Replace entirely with WebLLM's `WebWorkerMLCEngineHandler`.

```typescript
/**
 * LLM Web Worker — WebLLM inference engine (Phase 6 migration).
 *
 * Runs @mlc-ai/web-llm's WebWorkerMLCEngineHandler. The main thread
 * communicates via WebWorkerMLCEngine (in browser.ts), which sends
 * structured messages to this worker.
 *
 * WebLLM compiles models to WebGPU shaders using TVM; inference runs
 * entirely on the GPU. No CPU/WASM fallback — WebGPU is required.
 *
 * Replaces Phase 4's Transformers.js + SmolLM2 worker.
 */
import { WebWorkerMLCEngineHandler } from '@mlc-ai/web-llm';

const handler = new WebWorkerMLCEngineHandler();

self.onmessage = (msg: MessageEvent) => {
  handler.onmessage(msg);
};
```

That's the entire worker. WebLLM's handler manages model loading, inference, and progress reporting internally. The main-thread `WebWorkerMLCEngine` sends messages to this handler.

**3. Rewrite browser adapter (`src/ai/adapters/browser.ts`):**

Replace the Transformers.js pipeline integration with WebLLM's `CreateWebWorkerMLCEngine`.

Key changes:
- Remove all Transformers.js imports and pipeline logic
- Import `CreateWebWorkerMLCEngine` from `@mlc-ai/web-llm`
- Engine is created lazily on first call (or on explicit init)
- Use `engine.chat.completions.create()` for inference
- Support `response_format: { type: "json_object", schema }` for structured JSON
- Report model download progress via `initProgressCallback`

```typescript
/**
 * BrowserAdapter — local AI via WebLLM (Phase 6 migration).
 *
 * Uses @mlc-ai/web-llm's WebWorkerMLCEngine for GPU-accelerated inference
 * with guaranteed structured JSON output via XGrammar.
 *
 * Replaces Phase 4's Transformers.js + SmolLM2 BrowserAdapter.
 */
import { CreateWebWorkerMLCEngine, type MLCEngine, type InitProgressReport } from '@mlc-ai/web-llm';
import type { AIAdapter, AIAdapterResponse, AIProviderStatus } from './adapter';

// Model presets
export const WEBLLM_MODELS = [
  { id: 'Llama-3.2-1B-Instruct-q4f16_1-MLC', label: '1B (Low VRAM ~900MB)', vram: '~900MB' },
  { id: 'Llama-3.2-3B-Instruct-q4f16_1-MLC', label: '3B (Default ~2.2GB)', vram: '~2.2GB' },
  { id: 'Phi-3.5-mini-instruct-q4f16_1-MLC', label: '3.8B (High VRAM ~3.7GB)', vram: '~3.7GB' },
] as const;

export const DEFAULT_MODEL_ID = 'Llama-3.2-3B-Instruct-q4f16_1-MLC';
```

BrowserAdapter class:
- Constructor takes `modelId: string`, `onStatusChange?: (status: AIProviderStatus) => void`, `onDownloadProgress?: (progress: number) => void`
- `init()`: Creates the engine using `CreateWebWorkerMLCEngine` with a Worker pointing to the rewritten `llm-worker.ts`. Passes `initProgressCallback` to report download progress (progress.progress is 0-1, multiply by 100 for percentage). Sets status to 'loading' during init, 'available' on success, 'error' on failure.
- `execute(request)`: Uses `engine.chat.completions.create()` with messages array. If the request includes a `jsonSchema` field, pass `response_format: { type: "json_object", schema: JSON.stringify(jsonSchema) }`. Returns the response content as `AIAdapterResponse`.
- `terminate()`: Calls `engine.unload()` if engine exists.

```typescript
export class BrowserAdapter implements AIAdapter {
  private engine: MLCEngine | null = null;
  private modelId: string;
  private onStatusChange?: (status: AIProviderStatus) => void;
  private onDownloadProgress?: (progress: number) => void;

  constructor(
    modelId: string = DEFAULT_MODEL_ID,
    onStatusChange?: (status: AIProviderStatus) => void,
    onDownloadProgress?: (progress: number) => void,
  ) {
    this.modelId = modelId;
    this.onStatusChange = onStatusChange;
    this.onDownloadProgress = onDownloadProgress;
  }

  async init(): Promise<void> {
    this.onStatusChange?.('loading');
    try {
      const worker = new Worker(
        new URL('../llm-worker.ts', import.meta.url),
        { type: 'module' },
      );

      this.engine = await CreateWebWorkerMLCEngine(worker, this.modelId, {
        initProgressCallback: (report: InitProgressReport) => {
          // report.progress is 0-1
          this.onDownloadProgress?.(Math.round(report.progress * 100));
        },
      });

      this.onStatusChange?.('available');
    } catch (err) {
      console.error('[BrowserAdapter] WebLLM init failed:', err);
      this.onStatusChange?.('error');
      throw err;
    }
  }

  async execute(request: { requestId: string; prompt: string; maxTokens?: number; jsonSchema?: Record<string, unknown>; signal?: AbortSignal }): Promise<AIAdapterResponse> {
    if (!this.engine) {
      throw new Error('BrowserAdapter not initialized — call init() first');
    }

    const completionParams: Record<string, unknown> = {
      messages: [
        { role: 'system', content: 'You are a helpful GTD productivity assistant.' },
        { role: 'user', content: request.prompt },
      ],
      max_tokens: request.maxTokens ?? 512,
      temperature: 0.3,
    };

    // Structured JSON output via XGrammar
    if (request.jsonSchema) {
      completionParams.response_format = {
        type: 'json_object',
        schema: JSON.stringify(request.jsonSchema),
      };
    }

    const reply = await this.engine.chat.completions.create(completionParams as any);
    const content = reply.choices[0]?.message?.content ?? '';

    return {
      requestId: request.requestId,
      content,
      tokenCount: reply.usage?.total_tokens,
    };
  }

  async terminate(): Promise<void> {
    if (this.engine) {
      await (this.engine as any).unload?.();
      this.engine = null;
    }
    this.onStatusChange?.('disabled');
  }

  getStatus(): AIProviderStatus {
    return this.engine ? 'available' : 'disabled';
  }
}
```

**4. Update store.ts `activateBrowserLLM` to pass model ID:**

In `store.ts`, the `activateBrowserLLM` function creates a BrowserAdapter. Update it to read the selected model ID from AI settings:

Find the existing `activateBrowserLLM` function and update the BrowserAdapter constructor call to pass the selectedModelId:
```typescript
// In activateBrowserLLM:
const modelId = state.llmModelId || DEFAULT_MODEL_ID;
const adapter = new BrowserAdapter(
  modelId,
  (status) => setState('llmStatus', status),
  (progress) => setState('llmDownloadProgress', progress),
);
```

Import `DEFAULT_MODEL_ID` from the browser adapter.

**Important adapter interface check:** Verify the `AIAdapter` interface in `src/ai/adapters/adapter.ts`. The `execute` method signature may need to accept the optional `jsonSchema` field. If the interface doesn't have it, add it as optional:
```typescript
// In adapter.ts AIAdapter interface, if execute only takes prompt/maxTokens:
jsonSchema?: Record<string, unknown>;
```

Check and update the adapter interface and the router to pass jsonSchema through.
  </action>
  <verify>
- `pnpm list @mlc-ai/web-llm` shows 0.2.81
- `pnpm list @xenova/transformers` shows not installed (or whichever Transformers.js package was used)
- `npx tsc --noEmit` passes
- Grep `WebWorkerMLCEngineHandler` in llm-worker.ts confirms WebLLM worker
- Grep `CreateWebWorkerMLCEngine` in browser.ts confirms WebLLM engine creation
- Grep `response_format` in browser.ts confirms structured JSON support
- Grep `WEBLLM_MODELS` in browser.ts confirms model presets
  </verify>
  <done>
- @xenova/transformers (or equivalent) removed from dependencies
- @mlc-ai/web-llm@0.2.81 installed
- llm-worker.ts uses WebWorkerMLCEngineHandler (entire worker is ~10 lines)
- BrowserAdapter uses WebLLM's CreateWebWorkerMLCEngine with OpenAI-compatible chat API
- Structured JSON output supported via response_format with schema
- Model download progress reported to store
- activateBrowserLLM passes selected model ID to adapter
  </done>
</task>

<task type="auto">
  <name>Task 2: Model selector in AI Settings panel + settings persistence update</name>
  <files>
    src/ui/components/AISettingsPanel.tsx
    src/storage/ai-settings.ts
  </files>
  <action>
**1. Extend AISettings with selectedModelId (`src/storage/ai-settings.ts`):**

Add `selectedModelId` to the AISettings interface:
```typescript
export interface AISettings {
  // ... existing fields ...
  selectedModelId?: string;  // WebLLM model ID (default: Llama-3.2-3B)
}
```

No other changes needed — the existing load/save infrastructure handles arbitrary fields in the AISettings object.

**2. Add model selector to AISettingsPanel (`src/ui/components/AISettingsPanel.tsx`):**

Import the model presets from browser adapter:
```typescript
import { WEBLLM_MODELS, DEFAULT_MODEL_ID } from '../../ai/adapters/browser';
```

In the panel's "Browser LLM" section (near the browserLLMEnabled toggle), add a model selector dropdown:

```tsx
{/* Model selector — only shown when browser LLM is enabled */}
<Show when={state.browserLLMEnabled}>
  <div class="ai-settings-field">
    <label class="ai-settings-label" for="model-selector">
      Local Model
    </label>
    <select
      id="model-selector"
      class="ai-settings-select"
      value={state.llmModelId || DEFAULT_MODEL_ID}
      onChange={(e) => {
        const modelId = e.currentTarget.value;
        setState('llmModelId', modelId);
        // Persist the selection
        sendCommand({
          type: 'SAVE_AI_SETTINGS',
          payload: { selectedModelId: modelId },
        });
        // If LLM is currently active, reinitialize with new model
        // (user will need to reload or toggle off/on — model switch requires re-download)
      }}
    >
      <For each={WEBLLM_MODELS}>
        {(model) => (
          <option value={model.id}>
            {model.label}
          </option>
        )}
      </For>
    </select>
    <span class="ai-settings-hint">
      Larger models are more accurate but require more VRAM. WebGPU required.
    </span>
  </div>
</Show>
```

Add CSS for the select element in the existing AI settings panel styles section of layout.css (or inline if the panel already has a style pattern). The select should match the panel's dark theme:

```css
/* In the AI Settings Panel CSS section of layout.css */
.ai-settings-select {
  width: 100%;
  padding: 8px 12px;
  border-radius: 6px;
  border: 1px solid var(--border-primary);
  background: var(--bg-primary);
  color: var(--text-primary);
  font-size: 13px;
  cursor: pointer;
  appearance: none;
  background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='12' height='12' viewBox='0 0 12 12'%3E%3Cpath fill='%238b949e' d='M2 4l4 4 4-4'/%3E%3C/svg%3E");
  background-repeat: no-repeat;
  background-position: right 10px center;
}
.ai-settings-select:focus {
  outline: none;
  border-color: var(--accent);
}
```

**3. Wire selectedModelId in store.ts READY hydration:**

In the READY handler's aiSettings hydration block, add:
```typescript
if (s.selectedModelId !== undefined) setState('llmModelId', s.selectedModelId);
```

This ensures the persisted model selection is applied on reload. The `activateBrowserLLM` function (updated in Task 1) then reads `state.llmModelId` to create the adapter with the correct model.

**4. Update the WebGPU status message:**

In AISettingsPanel, where the browser LLM status is shown, update the messaging to reflect WebLLM (not Transformers.js). Change any references to "SmolLM2" to reference the selected model name. For example:
```typescript
// Where LLM model info is displayed:
const selectedModel = () => WEBLLM_MODELS.find(m => m.id === (state.llmModelId || DEFAULT_MODEL_ID));
// Display: selectedModel()?.label ?? 'Unknown model'
```

Remove any references to "SmolLM2", "135M", "360M", "Quality", "Fast", or Transformers.js-specific language.
  </action>
  <verify>
- `npx tsc --noEmit` passes
- Grep `selectedModelId` in ai-settings.ts confirms field added
- Grep `WEBLLM_MODELS` in AISettingsPanel.tsx confirms model selector imported
- Grep `model-selector` in AISettingsPanel.tsx confirms select element rendered
- Grep `SmolLM2` returns no results in AISettingsPanel.tsx (old references removed)
- Grep `.ai-settings-select` in layout.css confirms dropdown styling
  </verify>
  <done>
- AISettings interface has selectedModelId field
- AI Settings panel shows model selector dropdown with 1B/3B/3.8B options and VRAM guidance
- Model selection persisted via SAVE_AI_SETTINGS command
- Selected model ID hydrated on app reload
- activateBrowserLLM reads selectedModelId to create adapter with correct model
- All SmolLM2/Transformers.js references removed from settings UI
  </done>
</task>

</tasks>

<verification>
1. TypeScript compilation passes (`npx tsc --noEmit`)
2. `@mlc-ai/web-llm` is in package.json, Transformers.js package removed
3. LLM worker uses WebWorkerMLCEngineHandler
4. BrowserAdapter creates WebWorkerMLCEngine with selected model
5. Structured JSON via response_format is supported in execute()
6. Model selector dropdown renders in AI Settings with 3 options
7. Model selection persists across reloads
8. Download progress reported during model initialization
</verification>

<success_criteria>
- WebLLM fully replaces Transformers.js as the local AI engine
- Structured JSON output works via XGrammar-constrained generation
- Model selector allows users to choose model size based on their VRAM
- Model selection persists across sessions
- All Phase 5 triage functionality still works with the new engine
</success_criteria>

<output>
After completion, create `.planning/phases/06-review-pre-analysis/06-03-SUMMARY.md`
</output>
